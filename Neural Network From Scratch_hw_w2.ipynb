{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Neural Network from Scratch\n",
    "\n",
    "https://towardsdatascience.com/math-neural-network-from-scratch-in-python-d6da9f29ce65 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Abstract Base Class : Layer\n",
    "The abstract class Layer, which all other layers will inherit from, handles simple properties which are an input, an output, and both a forward and backward methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Base class\n",
    "class Layer:\n",
    "    def __init__(self):\n",
    "        self.input = None\n",
    "        self.output = None\n",
    "\n",
    "    # computes the output Y of a layer for a given input X\n",
    "    def forward_propagation(self, input):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    # computes dE/dX for a given dE/dY (and update parameters if any)\n",
    "    def backward_propagation(self, output_error, learning_rate):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the abstract class above, backward_propagation function has an extra parameter, learning_rate, which is controlling the amount of learning/updating parameters using gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backward Propagation\n",
    "Suppose we have a matrix containing the derivative of the error with respect to that layer’s output: $\\frac{\\partial E}{\\partial Y}$\n",
    "\n",
    "We need :\n",
    "- The derivative of the error with respect to the parameters ($\\frac{\\partial E}{\\partial W}$, $\\frac{\\partial E}{\\partial B}$)\n",
    "- The derivative of the error with respect to the input ($\\frac{\\partial E}{\\partial X}$)\n",
    "\n",
    "Let's calculate $\\frac{\\partial E}{\\partial W}$. This matrix should be the same size as $W$ itself : \n",
    "\n",
    "$i x j$ where $i$ is the number of input neurons and $j$ the number of output neurons. We need one gradient for every weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coding the Fully Connected Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from layer import Layer\n",
    "import numpy as np\n",
    "\n",
    "# inherit from base class Layer\n",
    "class FCLayer(Layer):\n",
    "    # input_size = number of input neurons\n",
    "    # output_size = number of edges that connects to neurons in next layer\n",
    "    def __init__(self, input_size, output_size):\n",
    "        self.weights = np.random.rand(input_size, output_size) - 0.5\n",
    "        self.bias = np.random.rand(1, output_size) - 0.5\n",
    "\n",
    "    # returns output for a given input\n",
    "    def forward_propagation(self, input_data):\n",
    "        self.input = input_data\n",
    "        self.output = np.dot(self.input, self.weights) + self.bias\n",
    "        return self.output\n",
    "\n",
    "    # computes dE/dW, dE/dB for a given output_error=dE/dY. Returns input_error=dE/dX.\n",
    "    def backward_propagation(self, output_error, learning_rate):\n",
    "        input_error = np.dot(output_error, self.weights.T)\n",
    "        weights_error = np.dot(self.input.T, output_error)\n",
    "        # dBias = output_error\n",
    "\n",
    "        # update parameters\n",
    "        self.weights -= learning_rate * weights_error\n",
    "        self.bias -= learning_rate * output_error\n",
    "        return input_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation Layer\n",
    "All the calculation we did until now were completely linear, may not learn well. We need to add non-linearity to the model by applying non-linear functions to the output of some layers.\n",
    "\n",
    "Now we need to redo the whole process for this new type of layer!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from layer import Layer\n",
    "\n",
    "# inherit from base class Layer\n",
    "class ActivationLayer(Layer):\n",
    "    def __init__(self, activation, activation_prime):\n",
    "        self.activation = activation\n",
    "        self.activation_prime = activation_prime\n",
    "\n",
    "    # returns the activated input\n",
    "    def forward_propagation(self, input_data):\n",
    "        self.input = input_data\n",
    "        self.output = self.activation(self.input)\n",
    "        return self.output\n",
    "\n",
    "    # Returns input_error=dE/dX for a given output_error=dE/dY.\n",
    "    # learning_rate is not used because there is no \"learnable\" parameters.\n",
    "    def backward_propagation(self, output_error, learning_rate):\n",
    "        return self.activation_prime(self.input) * output_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also write some activation functions and their derivatives in a separate file. These will be used later to create an ActivationLayer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# activation function and its derivative\n",
    "def tanh(x):\n",
    "    return np.tanh(x);\n",
    "\n",
    "def tanh_prime(x):\n",
    "    return 1-np.tanh(x)**2;\n",
    "\n",
    "# SIGMOID activation function and its derivative\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "def sigmoid_prime(x):\n",
    "    return sigmoid(x) * (1 - sigmoid(x))\n",
    "\n",
    "# ReLU activation function and its derivative\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def relu_prime(x):\n",
    "    x[x<=0] = 0\n",
    "    x[x>0] = 1\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Function\n",
    "Until now, for a given layer, we supposed that ∂E/∂Y was given (by the next layer). But what happens to the last layer? How does it get ∂E/∂Y? We simply give it manually, and it depends on how we define the error.\n",
    "The error of the network, which measures how good or bad the network did for a given input data, is defined by you. \n",
    "\n",
    "There are many ways to define the error, and one of the most known is called MSE — Mean Squared Error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "# loss function and its derivative\n",
    "def mse(y_true, y_pred):\n",
    "    return np.mean(np.power(y_true-y_pred, 2));\n",
    "\n",
    "def mse_prime(y_true, y_pred):\n",
    "    return 2*(y_pred-y_true)/y_true.size;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network Class\n",
    "Almost done ! We are going to make a Network class to create neural networks very easily using the building blocks we have prepared so far.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of a function for calculating softmax for a list of numbers\n",
    "from numpy import exp\n",
    " \n",
    "# calculate the softmax of a vector\n",
    "def softmax(vector):\n",
    "    e = exp(vector)\n",
    "    return e / e.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network:\n",
    "    def __init__(self):\n",
    "        self.layers = []\n",
    "        self.loss = None\n",
    "        self.loss_prime = None\n",
    "\n",
    "    # add layer to network\n",
    "    def add(self, layer):\n",
    "        self.layers.append(layer)\n",
    "\n",
    "    # set loss to use\n",
    "    def use(self, loss, loss_prime):\n",
    "        self.loss = loss\n",
    "        self.loss_prime = loss_prime\n",
    "\n",
    "        \n",
    "    # predict output for given input\n",
    "    def predict(self, input_data):\n",
    "        # sample dimension first\n",
    "        samples = len(input_data)\n",
    "        result = []\n",
    "\n",
    "        # run network over all samples\n",
    "        for i in range(samples):\n",
    "            # forward propagation\n",
    "            output = input_data[i]\n",
    "            for layer in self.layers:\n",
    "                output = layer.forward_propagation(output)\n",
    "            result.append(output)\n",
    "\n",
    "        return result\n",
    "\n",
    "    # train the network \n",
    "    \n",
    "    def fit(self, x_train, y_train, epochs, learning_rate):\n",
    "        '''\n",
    "        Fit function does the training. \n",
    "        Training data is passed 1-by-1 through the network layers during forward propagation.\n",
    "        Loss (error) is calculated for each input and back propagation is performed via partial \n",
    "        derivatives on each layer.\n",
    "        '''\n",
    "        # sample dimension first\n",
    "        samples = len(x_train)\n",
    "\n",
    "        # training loop\n",
    "        for i in range(epochs):\n",
    "            err = 0\n",
    "            for j in range(samples):\n",
    "                # forward propagation\n",
    "                output = x_train[j]\n",
    "                for layer in self.layers:\n",
    "                    output = layer.forward_propagation(output)\n",
    "\n",
    "                # compute loss (for display purpose only)\n",
    "                err += self.loss(y_train[j], output)\n",
    "\n",
    "                # backward propagation\n",
    "                error = self.loss_prime(y_train[j], output)\n",
    "                for layer in reversed(self.layers):\n",
    "                    error = layer.backward_propagation(error, learning_rate)\n",
    "\n",
    "            # calculate average error on all samples\n",
    "            err /= samples\n",
    "            print('epoch %d/%d   error=%f' % (i+1, epochs, err))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building Neural Networks\n",
    "Finally ! We can use our class to create a neural network with as many layers as we want ! We are going to build two neural networks : a simple XOR and a MNIST solver.\n",
    "\n",
    "\n",
    "### Solve XOR\n",
    "Starting with XOR is always important as it’s a simple way to tell if the network is learning anything at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1/1000   error=0.273759\n",
      "epoch 2/1000   error=0.270950\n",
      "epoch 3/1000   error=0.268572\n",
      "epoch 4/1000   error=0.266562\n",
      "epoch 5/1000   error=0.264865\n",
      "epoch 6/1000   error=0.263434\n",
      "epoch 7/1000   error=0.262227\n",
      "epoch 8/1000   error=0.261210\n",
      "epoch 9/1000   error=0.260352\n",
      "epoch 10/1000   error=0.259629\n",
      "epoch 11/1000   error=0.259019\n",
      "epoch 12/1000   error=0.258503\n",
      "epoch 13/1000   error=0.258068\n",
      "epoch 14/1000   error=0.257699\n",
      "epoch 15/1000   error=0.257387\n",
      "epoch 16/1000   error=0.257121\n",
      "epoch 17/1000   error=0.256896\n",
      "epoch 18/1000   error=0.256704\n",
      "epoch 19/1000   error=0.256539\n",
      "epoch 20/1000   error=0.256399\n",
      "epoch 21/1000   error=0.256278\n",
      "epoch 22/1000   error=0.256174\n",
      "epoch 23/1000   error=0.256084\n",
      "epoch 24/1000   error=0.256005\n",
      "epoch 25/1000   error=0.255937\n",
      "epoch 26/1000   error=0.255877\n",
      "epoch 27/1000   error=0.255824\n",
      "epoch 28/1000   error=0.255777\n",
      "epoch 29/1000   error=0.255735\n",
      "epoch 30/1000   error=0.255698\n",
      "epoch 31/1000   error=0.255664\n",
      "epoch 32/1000   error=0.255633\n",
      "epoch 33/1000   error=0.255605\n",
      "epoch 34/1000   error=0.255579\n",
      "epoch 35/1000   error=0.255554\n",
      "epoch 36/1000   error=0.255532\n",
      "epoch 37/1000   error=0.255511\n",
      "epoch 38/1000   error=0.255490\n",
      "epoch 39/1000   error=0.255471\n",
      "epoch 40/1000   error=0.255453\n",
      "epoch 41/1000   error=0.255435\n",
      "epoch 42/1000   error=0.255418\n",
      "epoch 43/1000   error=0.255401\n",
      "epoch 44/1000   error=0.255385\n",
      "epoch 45/1000   error=0.255369\n",
      "epoch 46/1000   error=0.255354\n",
      "epoch 47/1000   error=0.255338\n",
      "epoch 48/1000   error=0.255323\n",
      "epoch 49/1000   error=0.255308\n",
      "epoch 50/1000   error=0.255293\n",
      "epoch 51/1000   error=0.255278\n",
      "epoch 52/1000   error=0.255263\n",
      "epoch 53/1000   error=0.255248\n",
      "epoch 54/1000   error=0.255233\n",
      "epoch 55/1000   error=0.255218\n",
      "epoch 56/1000   error=0.255203\n",
      "epoch 57/1000   error=0.255188\n",
      "epoch 58/1000   error=0.255172\n",
      "epoch 59/1000   error=0.255157\n",
      "epoch 60/1000   error=0.255141\n",
      "epoch 61/1000   error=0.255125\n",
      "epoch 62/1000   error=0.255109\n",
      "epoch 63/1000   error=0.255093\n",
      "epoch 64/1000   error=0.255076\n",
      "epoch 65/1000   error=0.255062\n",
      "epoch 66/1000   error=0.255046\n",
      "epoch 67/1000   error=0.255030\n",
      "epoch 68/1000   error=0.255018\n",
      "epoch 69/1000   error=0.254999\n",
      "epoch 70/1000   error=0.254986\n",
      "epoch 71/1000   error=0.254968\n",
      "epoch 72/1000   error=0.254954\n",
      "epoch 73/1000   error=0.254936\n",
      "epoch 74/1000   error=0.254922\n",
      "epoch 75/1000   error=0.254904\n",
      "epoch 76/1000   error=0.254885\n",
      "epoch 77/1000   error=0.254875\n",
      "epoch 78/1000   error=0.254852\n",
      "epoch 79/1000   error=0.254840\n",
      "epoch 80/1000   error=0.254817\n",
      "epoch 81/1000   error=0.254805\n",
      "epoch 82/1000   error=0.254782\n",
      "epoch 83/1000   error=0.254769\n",
      "epoch 84/1000   error=0.254746\n",
      "epoch 85/1000   error=0.254731\n",
      "epoch 86/1000   error=0.254708\n",
      "epoch 87/1000   error=0.254693\n",
      "epoch 88/1000   error=0.254670\n",
      "epoch 89/1000   error=0.254646\n",
      "epoch 90/1000   error=0.254638\n",
      "epoch 91/1000   error=0.254606\n",
      "epoch 92/1000   error=0.254596\n",
      "epoch 93/1000   error=0.254563\n",
      "epoch 94/1000   error=0.254553\n",
      "epoch 95/1000   error=0.254520\n",
      "epoch 96/1000   error=0.254508\n",
      "epoch 97/1000   error=0.254474\n",
      "epoch 98/1000   error=0.254462\n",
      "epoch 99/1000   error=0.254427\n",
      "epoch 100/1000   error=0.254413\n",
      "epoch 101/1000   error=0.254378\n",
      "epoch 102/1000   error=0.254362\n",
      "epoch 103/1000   error=0.254326\n",
      "epoch 104/1000   error=0.254309\n",
      "epoch 105/1000   error=0.254272\n",
      "epoch 106/1000   error=0.254254\n",
      "epoch 107/1000   error=0.254216\n",
      "epoch 108/1000   error=0.254178\n",
      "epoch 109/1000   error=0.254178\n",
      "epoch 110/1000   error=0.254117\n",
      "epoch 111/1000   error=0.254116\n",
      "epoch 112/1000   error=0.254053\n",
      "epoch 113/1000   error=0.254051\n",
      "epoch 114/1000   error=0.253986\n",
      "epoch 115/1000   error=0.253983\n",
      "epoch 116/1000   error=0.253916\n",
      "epoch 117/1000   error=0.253911\n",
      "epoch 118/1000   error=0.253842\n",
      "epoch 119/1000   error=0.253836\n",
      "epoch 120/1000   error=0.253764\n",
      "epoch 121/1000   error=0.253757\n",
      "epoch 122/1000   error=0.253682\n",
      "epoch 123/1000   error=0.253673\n",
      "epoch 124/1000   error=0.253595\n",
      "epoch 125/1000   error=0.253585\n",
      "epoch 126/1000   error=0.253504\n",
      "epoch 127/1000   error=0.253491\n",
      "epoch 128/1000   error=0.253407\n",
      "epoch 129/1000   error=0.253393\n",
      "epoch 130/1000   error=0.253305\n",
      "epoch 131/1000   error=0.253289\n",
      "epoch 132/1000   error=0.253196\n",
      "epoch 133/1000   error=0.253178\n",
      "epoch 134/1000   error=0.253082\n",
      "epoch 135/1000   error=0.253062\n",
      "epoch 136/1000   error=0.252960\n",
      "epoch 137/1000   error=0.252840\n",
      "epoch 138/1000   error=0.252905\n",
      "epoch 139/1000   error=0.252798\n",
      "epoch 140/1000   error=0.252669\n",
      "epoch 141/1000   error=0.252737\n",
      "epoch 142/1000   error=0.252625\n",
      "epoch 143/1000   error=0.252487\n",
      "epoch 144/1000   error=0.252558\n",
      "epoch 145/1000   error=0.252441\n",
      "epoch 146/1000   error=0.252292\n",
      "epoch 147/1000   error=0.252286\n",
      "epoch 148/1000   error=0.252329\n",
      "epoch 149/1000   error=0.252117\n",
      "epoch 150/1000   error=0.252044\n",
      "epoch 151/1000   error=0.252118\n",
      "epoch 152/1000   error=0.251899\n",
      "epoch 153/1000   error=0.251818\n",
      "epoch 154/1000   error=0.251891\n",
      "epoch 155/1000   error=0.251666\n",
      "epoch 156/1000   error=0.251576\n",
      "epoch 157/1000   error=0.251647\n",
      "epoch 158/1000   error=0.251416\n",
      "epoch 159/1000   error=0.251316\n",
      "epoch 160/1000   error=0.251142\n",
      "epoch 161/1000   error=0.251336\n",
      "epoch 162/1000   error=0.251104\n",
      "epoch 163/1000   error=0.250855\n",
      "epoch 164/1000   error=0.251047\n",
      "epoch 165/1000   error=0.250813\n",
      "epoch 166/1000   error=0.250547\n",
      "epoch 167/1000   error=0.250735\n",
      "epoch 168/1000   error=0.250502\n",
      "epoch 169/1000   error=0.250217\n",
      "epoch 170/1000   error=0.250398\n",
      "epoch 171/1000   error=0.250168\n",
      "epoch 172/1000   error=0.249933\n",
      "epoch 173/1000   error=0.249807\n",
      "epoch 174/1000   error=0.249976\n",
      "epoch 175/1000   error=0.249564\n",
      "epoch 176/1000   error=0.249423\n",
      "epoch 177/1000   error=0.249575\n",
      "epoch 178/1000   error=0.249168\n",
      "epoch 179/1000   error=0.249011\n",
      "epoch 180/1000   error=0.248720\n",
      "epoch 181/1000   error=0.249071\n",
      "epoch 182/1000   error=0.248686\n",
      "epoch 183/1000   error=0.248265\n",
      "epoch 184/1000   error=0.248591\n",
      "epoch 185/1000   error=0.248227\n",
      "epoch 186/1000   error=0.247778\n",
      "epoch 187/1000   error=0.247830\n",
      "epoch 188/1000   error=0.247996\n",
      "epoch 189/1000   error=0.247381\n",
      "epoch 190/1000   error=0.247188\n",
      "epoch 191/1000   error=0.247422\n",
      "epoch 192/1000   error=0.246839\n",
      "epoch 193/1000   error=0.246625\n",
      "epoch 194/1000   error=0.246214\n",
      "epoch 195/1000   error=0.246707\n",
      "epoch 196/1000   error=0.246194\n",
      "epoch 197/1000   error=0.245594\n",
      "epoch 198/1000   error=0.245691\n",
      "epoch 199/1000   error=0.245921\n",
      "epoch 200/1000   error=0.245110\n",
      "epoch 201/1000   error=0.244858\n",
      "epoch 202/1000   error=0.245159\n",
      "epoch 203/1000   error=0.243646\n",
      "epoch 204/1000   error=0.244951\n",
      "epoch 205/1000   error=0.243623\n",
      "epoch 206/1000   error=0.244199\n",
      "epoch 207/1000   error=0.243628\n",
      "epoch 208/1000   error=0.243068\n",
      "epoch 209/1000   error=0.242765\n",
      "epoch 210/1000   error=0.243159\n",
      "epoch 211/1000   error=0.241314\n",
      "epoch 212/1000   error=0.242911\n",
      "epoch 213/1000   error=0.241307\n",
      "epoch 214/1000   error=0.241522\n",
      "epoch 215/1000   error=0.241837\n",
      "epoch 216/1000   error=0.240694\n",
      "epoch 217/1000   error=0.240324\n",
      "epoch 218/1000   error=0.239625\n",
      "epoch 219/1000   error=0.240517\n",
      "epoch 220/1000   error=0.239698\n",
      "epoch 221/1000   error=0.238963\n",
      "epoch 222/1000   error=0.238560\n",
      "epoch 223/1000   error=0.239064\n",
      "epoch 224/1000   error=0.236751\n",
      "epoch 225/1000   error=0.238726\n",
      "epoch 226/1000   error=0.237099\n",
      "epoch 227/1000   error=0.236635\n",
      "epoch 228/1000   error=0.237215\n",
      "epoch 229/1000   error=0.234676\n",
      "epoch 230/1000   error=0.236833\n",
      "epoch 231/1000   error=0.235085\n",
      "epoch 232/1000   error=0.234551\n",
      "epoch 233/1000   error=0.233623\n",
      "epoch 234/1000   error=0.234866\n",
      "epoch 235/1000   error=0.233867\n",
      "epoch 236/1000   error=0.232899\n",
      "epoch 237/1000   error=0.232319\n",
      "epoch 238/1000   error=0.231307\n",
      "epoch 239/1000   error=0.231932\n",
      "epoch 240/1000   error=0.232163\n",
      "epoch 241/1000   error=0.228921\n",
      "epoch 242/1000   error=0.231633\n",
      "epoch 243/1000   error=0.229532\n",
      "epoch 244/1000   error=0.228872\n",
      "epoch 245/1000   error=0.227742\n",
      "epoch 246/1000   error=0.228541\n",
      "epoch 247/1000   error=0.226650\n",
      "epoch 248/1000   error=0.227486\n",
      "epoch 249/1000   error=0.227850\n",
      "epoch 250/1000   error=0.223945\n",
      "epoch 251/1000   error=0.227154\n",
      "epoch 252/1000   error=0.224767\n",
      "epoch 253/1000   error=0.224015\n",
      "epoch 254/1000   error=0.223633\n",
      "epoch 255/1000   error=0.222851\n",
      "epoch 256/1000   error=0.222497\n",
      "epoch 257/1000   error=0.221681\n",
      "epoch 258/1000   error=0.220330\n",
      "epoch 259/1000   error=0.221545\n",
      "epoch 260/1000   error=0.219144\n",
      "epoch 261/1000   error=0.220393\n",
      "epoch 262/1000   error=0.217947\n",
      "epoch 263/1000   error=0.219230\n",
      "epoch 264/1000   error=0.216739\n",
      "epoch 265/1000   error=0.218053\n",
      "epoch 266/1000   error=0.215519\n",
      "epoch 267/1000   error=0.216863\n",
      "epoch 268/1000   error=0.214284\n",
      "epoch 269/1000   error=0.215658\n",
      "epoch 270/1000   error=0.213035\n",
      "epoch 271/1000   error=0.214439\n",
      "epoch 272/1000   error=0.212877\n",
      "epoch 273/1000   error=0.212085\n",
      "epoch 274/1000   error=0.211643\n",
      "epoch 275/1000   error=0.210791\n",
      "epoch 276/1000   error=0.210395\n",
      "epoch 277/1000   error=0.209480\n",
      "epoch 278/1000   error=0.209133\n",
      "epoch 279/1000   error=0.205738\n",
      "epoch 280/1000   error=0.207943\n",
      "epoch 281/1000   error=0.209070\n",
      "epoch 282/1000   error=0.204830\n",
      "epoch 283/1000   error=0.204755\n",
      "epoch 284/1000   error=0.206788\n",
      "epoch 285/1000   error=0.204989\n",
      "epoch 286/1000   error=0.201109\n",
      "epoch 287/1000   error=0.203764\n",
      "epoch 288/1000   error=0.204808\n",
      "epoch 289/1000   error=0.200707\n",
      "epoch 290/1000   error=0.202226\n",
      "epoch 291/1000   error=0.200778\n",
      "epoch 292/1000   error=0.200808\n",
      "epoch 293/1000   error=0.197457\n",
      "epoch 294/1000   error=0.199372\n",
      "epoch 295/1000   error=0.199431\n",
      "epoch 296/1000   error=0.197743\n",
      "epoch 297/1000   error=0.197932\n",
      "epoch 298/1000   error=0.194812\n",
      "epoch 299/1000   error=0.196348\n",
      "epoch 300/1000   error=0.196568\n",
      "epoch 301/1000   error=0.192560\n",
      "epoch 302/1000   error=0.194905\n",
      "epoch 303/1000   error=0.195170\n",
      "epoch 304/1000   error=0.190676\n",
      "epoch 305/1000   error=0.193395\n",
      "epoch 306/1000   error=0.193727\n",
      "epoch 307/1000   error=0.189040\n",
      "epoch 308/1000   error=0.191829\n",
      "epoch 309/1000   error=0.192247\n",
      "epoch 310/1000   error=0.187576\n",
      "epoch 311/1000   error=0.190219\n",
      "epoch 312/1000   error=0.190739\n",
      "epoch 313/1000   error=0.186237\n",
      "epoch 314/1000   error=0.188576\n",
      "epoch 315/1000   error=0.189212\n",
      "epoch 316/1000   error=0.184990\n",
      "epoch 317/1000   error=0.186909\n",
      "epoch 318/1000   error=0.187672\n",
      "epoch 319/1000   error=0.183811\n",
      "epoch 320/1000   error=0.185223\n",
      "epoch 321/1000   error=0.186121\n",
      "epoch 322/1000   error=0.182683\n",
      "epoch 323/1000   error=0.183522\n",
      "epoch 324/1000   error=0.184562\n",
      "epoch 325/1000   error=0.181593\n",
      "epoch 326/1000   error=0.182693\n",
      "epoch 327/1000   error=0.180141\n",
      "epoch 328/1000   error=0.182701\n",
      "epoch 329/1000   error=0.179039\n",
      "epoch 330/1000   error=0.180418\n",
      "epoch 331/1000   error=0.181703\n",
      "epoch 332/1000   error=0.178363\n",
      "epoch 333/1000   error=0.179730\n",
      "epoch 334/1000   error=0.177123\n",
      "epoch 335/1000   error=0.179269\n",
      "epoch 336/1000   error=0.176164\n",
      "epoch 337/1000   error=0.176901\n",
      "epoch 338/1000   error=0.178529\n",
      "epoch 339/1000   error=0.175593\n",
      "epoch 340/1000   error=0.176528\n",
      "epoch 341/1000   error=0.174419\n",
      "epoch 342/1000   error=0.175583\n",
      "epoch 343/1000   error=0.173501\n",
      "epoch 344/1000   error=0.175126\n",
      "epoch 345/1000   error=0.172485\n",
      "epoch 346/1000   error=0.173874\n",
      "epoch 347/1000   error=0.171663\n",
      "epoch 348/1000   error=0.173523\n",
      "epoch 349/1000   error=0.170711\n",
      "epoch 350/1000   error=0.171989\n",
      "epoch 351/1000   error=0.169923\n",
      "epoch 352/1000   error=0.171790\n",
      "epoch 353/1000   error=0.168989\n",
      "epoch 354/1000   error=0.169989\n",
      "epoch 355/1000   error=0.168204\n",
      "epoch 356/1000   error=0.169964\n",
      "epoch 357/1000   error=0.167265\n",
      "epoch 358/1000   error=0.165965\n",
      "epoch 359/1000   error=0.168481\n",
      "epoch 360/1000   error=0.167894\n",
      "epoch 361/1000   error=0.165673\n",
      "epoch 362/1000   error=0.164338\n",
      "epoch 363/1000   error=0.166052\n",
      "epoch 364/1000   error=0.165878\n",
      "epoch 365/1000   error=0.163930\n",
      "epoch 366/1000   error=0.162575\n",
      "epoch 367/1000   error=0.163648\n",
      "epoch 368/1000   error=0.163839\n",
      "epoch 369/1000   error=0.162065\n",
      "epoch 370/1000   error=0.160694\n",
      "epoch 371/1000   error=0.162007\n",
      "epoch 372/1000   error=0.159728\n",
      "epoch 373/1000   error=0.161439\n",
      "epoch 374/1000   error=0.158921\n",
      "epoch 375/1000   error=0.159626\n",
      "epoch 376/1000   error=0.157825\n",
      "epoch 377/1000   error=0.158680\n",
      "epoch 378/1000   error=0.156904\n",
      "epoch 379/1000   error=0.157293\n",
      "epoch 380/1000   error=0.155715\n",
      "epoch 381/1000   error=0.154376\n",
      "epoch 382/1000   error=0.156349\n",
      "epoch 383/1000   error=0.153517\n",
      "epoch 384/1000   error=0.154982\n",
      "epoch 385/1000   error=0.152357\n",
      "epoch 386/1000   error=0.153116\n",
      "epoch 387/1000   error=0.151319\n",
      "epoch 388/1000   error=0.152248\n",
      "epoch 389/1000   error=0.150014\n",
      "epoch 390/1000   error=0.148621\n",
      "epoch 391/1000   error=0.150351\n",
      "epoch 392/1000   error=0.149369\n",
      "epoch 393/1000   error=0.147585\n",
      "epoch 394/1000   error=0.146105\n",
      "epoch 395/1000   error=0.146994\n",
      "epoch 396/1000   error=0.144936\n",
      "epoch 397/1000   error=0.146644\n",
      "epoch 398/1000   error=0.143503\n",
      "epoch 399/1000   error=0.143900\n",
      "epoch 400/1000   error=0.141958\n",
      "epoch 401/1000   error=0.143299\n",
      "epoch 402/1000   error=0.140618\n",
      "epoch 403/1000   error=0.140821\n",
      "epoch 404/1000   error=0.138933\n",
      "epoch 405/1000   error=0.139728\n",
      "epoch 406/1000   error=0.137457\n",
      "epoch 407/1000   error=0.137683\n",
      "epoch 408/1000   error=0.135662\n",
      "epoch 409/1000   error=0.136119\n",
      "epoch 410/1000   error=0.134067\n",
      "epoch 411/1000   error=0.134430\n",
      "epoch 412/1000   error=0.132176\n",
      "epoch 413/1000   error=0.132428\n",
      "epoch 414/1000   error=0.130469\n",
      "epoch 415/1000   error=0.130211\n",
      "epoch 416/1000   error=0.128696\n",
      "epoch 417/1000   error=0.128932\n",
      "epoch 418/1000   error=0.126858\n",
      "epoch 419/1000   error=0.127864\n",
      "epoch 420/1000   error=0.124133\n",
      "epoch 421/1000   error=0.125417\n",
      "epoch 422/1000   error=0.123128\n",
      "epoch 423/1000   error=0.124564\n",
      "epoch 424/1000   error=0.120306\n",
      "epoch 425/1000   error=0.121880\n",
      "epoch 426/1000   error=0.121436\n",
      "epoch 427/1000   error=0.117888\n",
      "epoch 428/1000   error=0.119077\n",
      "epoch 429/1000   error=0.116657\n",
      "epoch 430/1000   error=0.118020\n",
      "epoch 431/1000   error=0.113819\n",
      "epoch 432/1000   error=0.115012\n",
      "epoch 433/1000   error=0.115370\n",
      "epoch 434/1000   error=0.111267\n",
      "epoch 435/1000   error=0.112986\n",
      "epoch 436/1000   error=0.109973\n",
      "epoch 437/1000   error=0.111172\n",
      "epoch 438/1000   error=0.109015\n",
      "epoch 439/1000   error=0.107496\n",
      "epoch 440/1000   error=0.107869\n",
      "epoch 441/1000   error=0.106212\n",
      "epoch 442/1000   error=0.104811\n",
      "epoch 443/1000   error=0.104763\n",
      "epoch 444/1000   error=0.103491\n",
      "epoch 445/1000   error=0.102041\n",
      "epoch 446/1000   error=0.102016\n",
      "epoch 447/1000   error=0.100519\n",
      "epoch 448/1000   error=0.099258\n",
      "epoch 449/1000   error=0.099290\n",
      "epoch 450/1000   error=0.095773\n",
      "epoch 451/1000   error=0.098541\n",
      "epoch 452/1000   error=0.096492\n",
      "epoch 453/1000   error=0.093112\n",
      "epoch 454/1000   error=0.095419\n",
      "epoch 455/1000   error=0.093831\n",
      "epoch 456/1000   error=0.090387\n",
      "epoch 457/1000   error=0.092453\n",
      "epoch 458/1000   error=0.091218\n",
      "epoch 459/1000   error=0.087654\n",
      "epoch 460/1000   error=0.089577\n",
      "epoch 461/1000   error=0.088635\n",
      "epoch 462/1000   error=0.084936\n",
      "epoch 463/1000   error=0.086773\n",
      "epoch 464/1000   error=0.086082\n",
      "epoch 465/1000   error=0.082246\n",
      "epoch 466/1000   error=0.084038\n",
      "epoch 467/1000   error=0.083563\n",
      "epoch 468/1000   error=0.079590\n",
      "epoch 469/1000   error=0.081371\n",
      "epoch 470/1000   error=0.080840\n",
      "epoch 471/1000   error=0.077181\n",
      "epoch 472/1000   error=0.078564\n",
      "epoch 473/1000   error=0.079055\n",
      "epoch 474/1000   error=0.076574\n",
      "epoch 475/1000   error=0.076475\n",
      "epoch 476/1000   error=0.073740\n",
      "epoch 477/1000   error=0.075019\n",
      "epoch 478/1000   error=0.074967\n",
      "epoch 479/1000   error=0.073013\n",
      "epoch 480/1000   error=0.071171\n",
      "epoch 481/1000   error=0.072010\n",
      "epoch 482/1000   error=0.071187\n",
      "epoch 483/1000   error=0.071435\n",
      "epoch 484/1000   error=0.069642\n",
      "epoch 485/1000   error=0.067861\n",
      "epoch 486/1000   error=0.068449\n",
      "epoch 487/1000   error=0.068241\n",
      "epoch 488/1000   error=0.066709\n",
      "epoch 489/1000   error=0.067200\n",
      "epoch 490/1000   error=0.064590\n",
      "epoch 491/1000   error=0.065195\n",
      "epoch 492/1000   error=0.065059\n",
      "epoch 493/1000   error=0.063532\n",
      "epoch 494/1000   error=0.063663\n",
      "epoch 495/1000   error=0.061143\n",
      "epoch 496/1000   error=0.062763\n",
      "epoch 497/1000   error=0.062026\n",
      "epoch 498/1000   error=0.060546\n",
      "epoch 499/1000   error=0.060662\n",
      "epoch 500/1000   error=0.058370\n",
      "epoch 501/1000   error=0.059318\n",
      "epoch 502/1000   error=0.058770\n",
      "epoch 503/1000   error=0.057962\n",
      "epoch 504/1000   error=0.057484\n",
      "epoch 505/1000   error=0.056222\n",
      "epoch 506/1000   error=0.055701\n",
      "epoch 507/1000   error=0.056944\n",
      "epoch 508/1000   error=0.055178\n",
      "epoch 509/1000   error=0.054940\n",
      "epoch 510/1000   error=0.053237\n",
      "epoch 511/1000   error=0.053504\n",
      "epoch 512/1000   error=0.053859\n",
      "epoch 513/1000   error=0.052267\n",
      "epoch 514/1000   error=0.052701\n",
      "epoch 515/1000   error=0.050405\n",
      "epoch 516/1000   error=0.052726\n",
      "epoch 517/1000   error=0.049516\n",
      "epoch 518/1000   error=0.050573\n",
      "epoch 519/1000   error=0.049687\n",
      "epoch 520/1000   error=0.049798\n",
      "epoch 521/1000   error=0.049031\n",
      "epoch 522/1000   error=0.047497\n",
      "epoch 523/1000   error=0.047908\n",
      "epoch 524/1000   error=0.047914\n",
      "epoch 525/1000   error=0.047190\n",
      "epoch 526/1000   error=0.046584\n",
      "epoch 527/1000   error=0.045460\n",
      "epoch 528/1000   error=0.046444\n",
      "epoch 529/1000   error=0.044872\n",
      "epoch 530/1000   error=0.044580\n",
      "epoch 531/1000   error=0.045155\n",
      "epoch 532/1000   error=0.044009\n",
      "epoch 533/1000   error=0.043903\n",
      "epoch 534/1000   error=0.042466\n",
      "epoch 535/1000   error=0.043786\n",
      "epoch 536/1000   error=0.041930\n",
      "epoch 537/1000   error=0.042081\n",
      "epoch 538/1000   error=0.042127\n",
      "epoch 539/1000   error=0.041629\n",
      "epoch 540/1000   error=0.040930\n",
      "epoch 541/1000   error=0.040188\n",
      "epoch 542/1000   error=0.040816\n",
      "epoch 543/1000   error=0.039672\n",
      "epoch 544/1000   error=0.039281\n",
      "epoch 545/1000   error=0.039874\n",
      "epoch 546/1000   error=0.038880\n",
      "epoch 547/1000   error=0.038307\n",
      "epoch 548/1000   error=0.038095\n",
      "epoch 549/1000   error=0.038115\n",
      "epoch 550/1000   error=0.037708\n",
      "epoch 551/1000   error=0.037769\n",
      "epoch 552/1000   error=0.036635\n",
      "epoch 553/1000   error=0.037055\n",
      "epoch 554/1000   error=0.035545\n",
      "epoch 555/1000   error=0.036514\n",
      "epoch 556/1000   error=0.035205\n",
      "epoch 557/1000   error=0.036182\n",
      "epoch 558/1000   error=0.034912\n",
      "epoch 559/1000   error=0.034419\n",
      "epoch 560/1000   error=0.034586\n",
      "epoch 561/1000   error=0.034420\n",
      "epoch 562/1000   error=0.034117\n",
      "epoch 563/1000   error=0.034133\n",
      "epoch 564/1000   error=0.033173\n",
      "epoch 565/1000   error=0.033347\n",
      "epoch 566/1000   error=0.032342\n",
      "epoch 567/1000   error=0.032849\n",
      "epoch 568/1000   error=0.032107\n",
      "epoch 569/1000   error=0.032585\n",
      "epoch 570/1000   error=0.031865\n",
      "epoch 571/1000   error=0.031692\n",
      "epoch 572/1000   error=0.031349\n",
      "epoch 573/1000   error=0.030749\n",
      "epoch 574/1000   error=0.030888\n",
      "epoch 575/1000   error=0.030522\n",
      "epoch 576/1000   error=0.030665\n",
      "epoch 577/1000   error=0.030295\n",
      "epoch 578/1000   error=0.029852\n",
      "epoch 579/1000   error=0.029784\n",
      "epoch 580/1000   error=0.029036\n",
      "epoch 581/1000   error=0.029347\n",
      "epoch 582/1000   error=0.028801\n",
      "epoch 583/1000   error=0.029131\n",
      "epoch 584/1000   error=0.028599\n",
      "epoch 585/1000   error=0.028366\n",
      "epoch 586/1000   error=0.028158\n",
      "epoch 587/1000   error=0.027319\n",
      "epoch 588/1000   error=0.028037\n",
      "epoch 589/1000   error=0.027113\n",
      "epoch 590/1000   error=0.027823\n",
      "epoch 591/1000   error=0.026952\n",
      "epoch 592/1000   error=0.026973\n",
      "epoch 593/1000   error=0.026689\n",
      "epoch 594/1000   error=0.026221\n",
      "epoch 595/1000   error=0.026323\n",
      "epoch 596/1000   error=0.026046\n",
      "epoch 597/1000   error=0.026155\n",
      "epoch 598/1000   error=0.025862\n",
      "epoch 599/1000   error=0.025519\n",
      "epoch 600/1000   error=0.025454\n",
      "epoch 601/1000   error=0.024863\n",
      "epoch 602/1000   error=0.025102\n",
      "epoch 603/1000   error=0.024694\n",
      "epoch 604/1000   error=0.024670\n",
      "epoch 605/1000   error=0.024786\n",
      "epoch 606/1000   error=0.024120\n",
      "epoch 607/1000   error=0.024266\n",
      "epoch 608/1000   error=0.023623\n",
      "epoch 609/1000   error=0.023931\n",
      "epoch 610/1000   error=0.023839\n",
      "epoch 611/1000   error=0.023350\n",
      "epoch 612/1000   error=0.023178\n",
      "epoch 613/1000   error=0.023416\n",
      "epoch 614/1000   error=0.023040\n",
      "epoch 615/1000   error=0.022573\n",
      "epoch 616/1000   error=0.022742\n",
      "epoch 617/1000   error=0.022575\n",
      "epoch 618/1000   error=0.022398\n",
      "epoch 619/1000   error=0.021978\n",
      "epoch 620/1000   error=0.022367\n",
      "epoch 621/1000   error=0.021965\n",
      "epoch 622/1000   error=0.021852\n",
      "epoch 623/1000   error=0.021676\n",
      "epoch 624/1000   error=0.021298\n",
      "epoch 625/1000   error=0.021405\n",
      "epoch 626/1000   error=0.021459\n",
      "epoch 627/1000   error=0.020959\n",
      "epoch 628/1000   error=0.020865\n",
      "epoch 629/1000   error=0.021005\n",
      "epoch 630/1000   error=0.020615\n",
      "epoch 631/1000   error=0.020360\n",
      "epoch 632/1000   error=0.020440\n",
      "epoch 633/1000   error=0.020570\n",
      "epoch 634/1000   error=0.020006\n",
      "epoch 635/1000   error=0.019982\n",
      "epoch 636/1000   error=0.020095\n",
      "epoch 637/1000   error=0.019876\n",
      "epoch 638/1000   error=0.019429\n",
      "epoch 639/1000   error=0.019433\n",
      "epoch 640/1000   error=0.019809\n",
      "epoch 641/1000   error=0.019053\n",
      "epoch 642/1000   error=0.019394\n",
      "epoch 643/1000   error=0.018949\n",
      "epoch 644/1000   error=0.019063\n",
      "epoch 645/1000   error=0.018829\n",
      "epoch 646/1000   error=0.018800\n",
      "epoch 647/1000   error=0.018409\n",
      "epoch 648/1000   error=0.018396\n",
      "epoch 649/1000   error=0.018727\n",
      "epoch 650/1000   error=0.018064\n",
      "epoch 651/1000   error=0.018143\n",
      "epoch 652/1000   error=0.018204\n",
      "epoch 653/1000   error=0.018044\n",
      "epoch 654/1000   error=0.017844\n",
      "epoch 655/1000   error=0.017806\n",
      "epoch 656/1000   error=0.017479\n",
      "epoch 657/1000   error=0.017450\n",
      "epoch 658/1000   error=0.017725\n",
      "epoch 659/1000   error=0.017161\n",
      "epoch 660/1000   error=0.017213\n",
      "epoch 661/1000   error=0.017266\n",
      "epoch 662/1000   error=0.017114\n",
      "epoch 663/1000   error=0.016763\n",
      "epoch 664/1000   error=0.016755\n",
      "epoch 665/1000   error=0.017074\n",
      "epoch 666/1000   error=0.016459\n",
      "epoch 667/1000   error=0.016557\n",
      "epoch 668/1000   error=0.016591\n",
      "epoch 669/1000   error=0.016460\n",
      "epoch 670/1000   error=0.016292\n",
      "epoch 671/1000   error=0.016254\n",
      "epoch 672/1000   error=0.015965\n",
      "epoch 673/1000   error=0.015950\n",
      "epoch 674/1000   error=0.016197\n",
      "epoch 675/1000   error=0.015690\n",
      "epoch 676/1000   error=0.015748\n",
      "epoch 677/1000   error=0.015786\n",
      "epoch 678/1000   error=0.015652\n",
      "epoch 679/1000   error=0.015517\n",
      "epoch 680/1000   error=0.015352\n",
      "epoch 681/1000   error=0.015317\n",
      "epoch 682/1000   error=0.015217\n",
      "epoch 683/1000   error=0.015349\n",
      "epoch 684/1000   error=0.015016\n",
      "epoch 685/1000   error=0.014998\n",
      "epoch 686/1000   error=0.015050\n",
      "epoch 687/1000   error=0.014797\n",
      "epoch 688/1000   error=0.014905\n",
      "epoch 689/1000   error=0.014667\n",
      "epoch 690/1000   error=0.014542\n",
      "epoch 691/1000   error=0.014579\n",
      "epoch 692/1000   error=0.014626\n",
      "epoch 693/1000   error=0.014341\n",
      "epoch 694/1000   error=0.014197\n",
      "epoch 695/1000   error=0.014469\n",
      "epoch 696/1000   error=0.014153\n",
      "epoch 697/1000   error=0.014159\n",
      "epoch 698/1000   error=0.014068\n",
      "epoch 699/1000   error=0.013880\n",
      "epoch 700/1000   error=0.013935\n",
      "epoch 701/1000   error=0.013856\n",
      "epoch 702/1000   error=0.013795\n",
      "epoch 703/1000   error=0.013599\n",
      "epoch 704/1000   error=0.013757\n",
      "epoch 705/1000   error=0.013591\n",
      "epoch 706/1000   error=0.013525\n",
      "epoch 707/1000   error=0.013460\n",
      "epoch 708/1000   error=0.013184\n",
      "epoch 709/1000   error=0.013419\n",
      "epoch 710/1000   error=0.013281\n",
      "epoch 711/1000   error=0.013138\n",
      "epoch 712/1000   error=0.013072\n",
      "epoch 713/1000   error=0.013149\n",
      "epoch 714/1000   error=0.013016\n",
      "epoch 715/1000   error=0.012858\n",
      "epoch 716/1000   error=0.012963\n",
      "epoch 717/1000   error=0.012655\n",
      "epoch 718/1000   error=0.012790\n",
      "epoch 719/1000   error=0.012774\n",
      "epoch 720/1000   error=0.012575\n",
      "epoch 721/1000   error=0.012535\n",
      "epoch 722/1000   error=0.012516\n",
      "epoch 723/1000   error=0.012504\n",
      "epoch 724/1000   error=0.012390\n",
      "epoch 725/1000   error=0.012370\n",
      "epoch 726/1000   error=0.012181\n",
      "epoch 727/1000   error=0.012167\n",
      "epoch 728/1000   error=0.012343\n",
      "epoch 729/1000   error=0.012001\n",
      "epoch 730/1000   error=0.012048\n",
      "epoch 731/1000   error=0.012066\n",
      "epoch 732/1000   error=0.011979\n",
      "epoch 733/1000   error=0.011900\n",
      "epoch 734/1000   error=0.011792\n",
      "epoch 735/1000   error=0.011723\n",
      "epoch 736/1000   error=0.011747\n",
      "epoch 737/1000   error=0.011787\n",
      "epoch 738/1000   error=0.011574\n",
      "epoch 739/1000   error=0.011480\n",
      "epoch 740/1000   error=0.011669\n",
      "epoch 741/1000   error=0.011445\n",
      "epoch 742/1000   error=0.011455\n",
      "epoch 743/1000   error=0.011388\n",
      "epoch 744/1000   error=0.011248\n",
      "epoch 745/1000   error=0.011300\n",
      "epoch 746/1000   error=0.011252\n",
      "epoch 747/1000   error=0.011151\n",
      "epoch 748/1000   error=0.011101\n",
      "epoch 749/1000   error=0.011160\n",
      "epoch 750/1000   error=0.011055\n",
      "epoch 751/1000   error=0.010934\n",
      "epoch 752/1000   error=0.011025\n",
      "epoch 753/1000   error=0.010860\n",
      "epoch 754/1000   error=0.010878\n",
      "epoch 755/1000   error=0.010732\n",
      "epoch 756/1000   error=0.010712\n",
      "epoch 757/1000   error=0.010857\n",
      "epoch 758/1000   error=0.010582\n",
      "epoch 759/1000   error=0.010623\n",
      "epoch 760/1000   error=0.010630\n",
      "epoch 761/1000   error=0.010560\n",
      "epoch 762/1000   error=0.010503\n",
      "epoch 763/1000   error=0.010410\n",
      "epoch 764/1000   error=0.010358\n",
      "epoch 765/1000   error=0.010379\n",
      "epoch 766/1000   error=0.010405\n",
      "epoch 767/1000   error=0.010239\n",
      "epoch 768/1000   error=0.010164\n",
      "epoch 769/1000   error=0.010305\n",
      "epoch 770/1000   error=0.010132\n",
      "epoch 771/1000   error=0.010138\n",
      "epoch 772/1000   error=0.010084\n",
      "epoch 773/1000   error=0.009985\n",
      "epoch 774/1000   error=0.009988\n",
      "epoch 775/1000   error=0.009995\n",
      "epoch 776/1000   error=0.009880\n",
      "epoch 777/1000   error=0.009859\n",
      "epoch 778/1000   error=0.009890\n",
      "epoch 779/1000   error=0.009783\n",
      "epoch 780/1000   error=0.009757\n",
      "epoch 781/1000   error=0.009749\n",
      "epoch 782/1000   error=0.009690\n",
      "epoch 783/1000   error=0.009660\n",
      "epoch 784/1000   error=0.009552\n",
      "epoch 785/1000   error=0.009545\n",
      "epoch 786/1000   error=0.009569\n",
      "epoch 787/1000   error=0.009511\n",
      "epoch 788/1000   error=0.009456\n",
      "epoch 789/1000   error=0.009463\n",
      "epoch 790/1000   error=0.009350\n",
      "epoch 791/1000   error=0.009411\n",
      "epoch 792/1000   error=0.009294\n",
      "epoch 793/1000   error=0.009235\n",
      "epoch 794/1000   error=0.009265\n",
      "epoch 795/1000   error=0.009223\n",
      "epoch 796/1000   error=0.009189\n",
      "epoch 797/1000   error=0.009099\n",
      "epoch 798/1000   error=0.009161\n",
      "epoch 799/1000   error=0.009060\n",
      "epoch 800/1000   error=0.009030\n",
      "epoch 801/1000   error=0.009066\n",
      "epoch 802/1000   error=0.008953\n",
      "epoch 803/1000   error=0.008897\n",
      "epoch 804/1000   error=0.008977\n",
      "epoch 805/1000   error=0.008789\n",
      "epoch 806/1000   error=0.008946\n",
      "epoch 807/1000   error=0.008752\n",
      "epoch 808/1000   error=0.008779\n",
      "epoch 809/1000   error=0.008782\n",
      "epoch 810/1000   error=0.008680\n",
      "epoch 811/1000   error=0.008741\n",
      "epoch 812/1000   error=0.008631\n",
      "epoch 813/1000   error=0.008640\n",
      "epoch 814/1000   error=0.008542\n",
      "epoch 815/1000   error=0.008578\n",
      "epoch 816/1000   error=0.008536\n",
      "epoch 817/1000   error=0.008462\n",
      "epoch 818/1000   error=0.008516\n",
      "epoch 819/1000   error=0.008428\n",
      "epoch 820/1000   error=0.008399\n",
      "epoch 821/1000   error=0.008433\n",
      "epoch 822/1000   error=0.008332\n",
      "epoch 823/1000   error=0.008338\n",
      "epoch 824/1000   error=0.008246\n",
      "epoch 825/1000   error=0.008239\n",
      "epoch 826/1000   error=0.008329\n",
      "epoch 827/1000   error=0.008153\n",
      "epoch 828/1000   error=0.008179\n",
      "epoch 829/1000   error=0.008181\n",
      "epoch 830/1000   error=0.008091\n",
      "epoch 831/1000   error=0.008145\n",
      "epoch 832/1000   error=0.008048\n",
      "epoch 833/1000   error=0.008056\n",
      "epoch 834/1000   error=0.007968\n",
      "epoch 835/1000   error=0.008003\n",
      "epoch 836/1000   error=0.007961\n",
      "epoch 837/1000   error=0.007951\n",
      "epoch 838/1000   error=0.007886\n",
      "epoch 839/1000   error=0.007869\n",
      "epoch 840/1000   error=0.007846\n",
      "epoch 841/1000   error=0.007870\n",
      "epoch 842/1000   error=0.007785\n",
      "epoch 843/1000   error=0.007786\n",
      "epoch 844/1000   error=0.007755\n",
      "epoch 845/1000   error=0.007649\n",
      "epoch 846/1000   error=0.007763\n",
      "epoch 847/1000   error=0.007638\n",
      "epoch 848/1000   error=0.007645\n",
      "epoch 849/1000   error=0.007650\n",
      "epoch 850/1000   error=0.007556\n",
      "epoch 851/1000   error=0.007610\n",
      "epoch 852/1000   error=0.007550\n",
      "epoch 853/1000   error=0.007535\n",
      "epoch 854/1000   error=0.007503\n",
      "epoch 855/1000   error=0.007407\n",
      "epoch 856/1000   error=0.007475\n",
      "epoch 857/1000   error=0.007464\n",
      "epoch 858/1000   error=0.007381\n",
      "epoch 859/1000   error=0.007372\n",
      "epoch 860/1000   error=0.007353\n",
      "epoch 861/1000   error=0.007334\n",
      "epoch 862/1000   error=0.007330\n",
      "epoch 863/1000   error=0.007292\n",
      "epoch 864/1000   error=0.007270\n",
      "epoch 865/1000   error=0.007177\n",
      "epoch 866/1000   error=0.007272\n",
      "epoch 867/1000   error=0.007165\n",
      "epoch 868/1000   error=0.007131\n",
      "epoch 869/1000   error=0.007210\n",
      "epoch 870/1000   error=0.007097\n",
      "epoch 871/1000   error=0.007134\n",
      "epoch 872/1000   error=0.007086\n",
      "epoch 873/1000   error=0.007037\n",
      "epoch 874/1000   error=0.007073\n",
      "epoch 875/1000   error=0.007005\n",
      "epoch 876/1000   error=0.006967\n",
      "epoch 877/1000   error=0.007016\n",
      "epoch 878/1000   error=0.006900\n",
      "epoch 879/1000   error=0.006938\n",
      "epoch 880/1000   error=0.006930\n",
      "epoch 881/1000   error=0.006888\n",
      "epoch 882/1000   error=0.006889\n",
      "epoch 883/1000   error=0.006825\n",
      "epoch 884/1000   error=0.006845\n",
      "epoch 885/1000   error=0.006807\n",
      "epoch 886/1000   error=0.006795\n",
      "epoch 887/1000   error=0.006737\n",
      "epoch 888/1000   error=0.006761\n",
      "epoch 889/1000   error=0.006709\n",
      "epoch 890/1000   error=0.006700\n",
      "epoch 891/1000   error=0.006676\n",
      "epoch 892/1000   error=0.006691\n",
      "epoch 893/1000   error=0.006645\n",
      "epoch 894/1000   error=0.006635\n",
      "epoch 895/1000   error=0.006614\n",
      "epoch 896/1000   error=0.006527\n",
      "epoch 897/1000   error=0.006637\n",
      "epoch 898/1000   error=0.006507\n",
      "epoch 899/1000   error=0.006530\n",
      "epoch 900/1000   error=0.006531\n",
      "epoch 901/1000   error=0.006463\n",
      "epoch 902/1000   error=0.006501\n",
      "epoch 903/1000   error=0.006448\n",
      "epoch 904/1000   error=0.006454\n",
      "epoch 905/1000   error=0.006420\n",
      "epoch 906/1000   error=0.006385\n",
      "epoch 907/1000   error=0.006358\n",
      "epoch 908/1000   error=0.006399\n",
      "epoch 909/1000   error=0.006295\n",
      "epoch 910/1000   error=0.006351\n",
      "epoch 911/1000   error=0.006308\n",
      "epoch 912/1000   error=0.006291\n",
      "epoch 913/1000   error=0.006288\n",
      "epoch 914/1000   error=0.006226\n",
      "epoch 915/1000   error=0.006274\n",
      "epoch 916/1000   error=0.006206\n",
      "epoch 917/1000   error=0.006212\n",
      "epoch 918/1000   error=0.006158\n",
      "epoch 919/1000   error=0.006181\n",
      "epoch 920/1000   error=0.006136\n",
      "epoch 921/1000   error=0.006130\n",
      "epoch 922/1000   error=0.006106\n",
      "epoch 923/1000   error=0.006123\n",
      "epoch 924/1000   error=0.006080\n",
      "epoch 925/1000   error=0.006066\n",
      "epoch 926/1000   error=0.006063\n",
      "epoch 927/1000   error=0.006011\n",
      "epoch 928/1000   error=0.006044\n",
      "epoch 929/1000   error=0.005960\n",
      "epoch 930/1000   error=0.006018\n",
      "epoch 931/1000   error=0.005947\n",
      "epoch 932/1000   error=0.005929\n",
      "epoch 933/1000   error=0.005957\n",
      "epoch 934/1000   error=0.005914\n",
      "epoch 935/1000   error=0.005891\n",
      "epoch 936/1000   error=0.005908\n",
      "epoch 937/1000   error=0.005868\n",
      "epoch 938/1000   error=0.005860\n",
      "epoch 939/1000   error=0.005846\n",
      "epoch 940/1000   error=0.005779\n",
      "epoch 941/1000   error=0.005847\n",
      "epoch 942/1000   error=0.005772\n",
      "epoch 943/1000   error=0.005776\n",
      "epoch 944/1000   error=0.005777\n",
      "epoch 945/1000   error=0.005726\n",
      "epoch 946/1000   error=0.005750\n",
      "epoch 947/1000   error=0.005712\n",
      "epoch 948/1000   error=0.005691\n",
      "epoch 949/1000   error=0.005709\n",
      "epoch 950/1000   error=0.005664\n",
      "epoch 951/1000   error=0.005636\n",
      "epoch 952/1000   error=0.005672\n",
      "epoch 953/1000   error=0.005591\n",
      "epoch 954/1000   error=0.005618\n",
      "epoch 955/1000   error=0.005611\n",
      "epoch 956/1000   error=0.005582\n",
      "epoch 957/1000   error=0.005582\n",
      "epoch 958/1000   error=0.005536\n",
      "epoch 959/1000   error=0.005556\n",
      "epoch 960/1000   error=0.005500\n",
      "epoch 961/1000   error=0.005524\n",
      "epoch 962/1000   error=0.005496\n",
      "epoch 963/1000   error=0.005498\n",
      "epoch 964/1000   error=0.005455\n",
      "epoch 965/1000   error=0.005455\n",
      "epoch 966/1000   error=0.005438\n",
      "epoch 967/1000   error=0.005432\n",
      "epoch 968/1000   error=0.005426\n",
      "epoch 969/1000   error=0.005377\n",
      "epoch 970/1000   error=0.005419\n",
      "epoch 971/1000   error=0.005336\n",
      "epoch 972/1000   error=0.005395\n",
      "epoch 973/1000   error=0.005328\n",
      "epoch 974/1000   error=0.005341\n",
      "epoch 975/1000   error=0.005311\n",
      "epoch 976/1000   error=0.005305\n",
      "epoch 977/1000   error=0.005285\n",
      "epoch 978/1000   error=0.005301\n",
      "epoch 979/1000   error=0.005265\n",
      "epoch 980/1000   error=0.005256\n",
      "epoch 981/1000   error=0.005252\n",
      "epoch 982/1000   error=0.005212\n",
      "epoch 983/1000   error=0.005228\n",
      "epoch 984/1000   error=0.005182\n",
      "epoch 985/1000   error=0.005217\n",
      "epoch 986/1000   error=0.005163\n",
      "epoch 987/1000   error=0.005150\n",
      "epoch 988/1000   error=0.005174\n",
      "epoch 989/1000   error=0.005131\n",
      "epoch 990/1000   error=0.005121\n",
      "epoch 991/1000   error=0.005129\n",
      "epoch 992/1000   error=0.005103\n",
      "epoch 993/1000   error=0.005093\n",
      "epoch 994/1000   error=0.005085\n",
      "epoch 995/1000   error=0.005033\n",
      "epoch 996/1000   error=0.005084\n",
      "epoch 997/1000   error=0.005026\n",
      "epoch 998/1000   error=0.005012\n",
      "epoch 999/1000   error=0.005053\n",
      "epoch 1000/1000   error=0.004987\n",
      "[array([[0.10745889]]), array([[0.94459106]]), array([[0.94352829]]), array([[0.04443573]])]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "#from network import Network\n",
    "#from fc_layer import FCLayer\n",
    "#from activation_layer import ActivationLayer\n",
    "#from activations import tanh, tanh_prime\n",
    "#from losses import mse, mse_prime\n",
    "\n",
    "# training data\n",
    "x_train = np.array([[[0,0]], [[0,1]], [[1,0]], [[1,1]]])\n",
    "y_train = np.array([[[0]], [[1]], [[1]], [[0]]])\n",
    "\n",
    "# network\n",
    "net = Network()\n",
    "net.add(FCLayer(2, 3))\n",
    "net.add(ActivationLayer(relu, relu_prime))\n",
    "net.add(FCLayer(3, 1))\n",
    "net.add(ActivationLayer(sigmoid, sigmoid_prime))\n",
    "\n",
    "# train\n",
    "net.use(mse, mse_prime)\n",
    "net.fit(x_train, y_train, epochs=1000, learning_rate=0.1)\n",
    "\n",
    "# test\n",
    "out = net.predict(x_train)\n",
    "print(out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solve MNIST\n",
    "We didn’t implemented the Convolutional Layer but this is not a problem. \n",
    "All we need to do is to reshape our data so that it can fit into a Fully Connected Layer.\n",
    "MNIST Dataset consists of images of digits from 0 to 9, of shape 28x28x1. \n",
    "The goal is to predict what digit is drawn on a picture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "epoch 1/35   error=0.099206\n",
      "epoch 2/35   error=0.067142\n",
      "epoch 3/35   error=0.054948\n",
      "epoch 4/35   error=0.048092\n",
      "epoch 5/35   error=0.043620\n",
      "epoch 6/35   error=0.040832\n",
      "epoch 7/35   error=0.039079\n",
      "epoch 8/35   error=0.037723\n",
      "epoch 9/35   error=0.036798\n",
      "epoch 10/35   error=0.036164\n",
      "epoch 11/35   error=0.035603\n",
      "epoch 12/35   error=0.035005\n",
      "epoch 13/35   error=0.034552\n",
      "epoch 14/35   error=0.034220\n",
      "epoch 15/35   error=0.033957\n",
      "epoch 16/35   error=0.033768\n",
      "epoch 17/35   error=0.033573\n",
      "epoch 18/35   error=0.033330\n",
      "epoch 19/35   error=0.033113\n",
      "epoch 20/35   error=0.032964\n",
      "epoch 21/35   error=0.032877\n",
      "epoch 22/35   error=0.032779\n",
      "epoch 23/35   error=0.032638\n",
      "epoch 24/35   error=0.032555\n",
      "epoch 25/35   error=0.032490\n",
      "epoch 26/35   error=0.032431\n",
      "epoch 27/35   error=0.032356\n",
      "epoch 28/35   error=0.032309\n",
      "epoch 29/35   error=0.032279\n",
      "epoch 30/35   error=0.032243\n",
      "epoch 31/35   error=0.032191\n",
      "epoch 32/35   error=0.032121\n",
      "epoch 33/35   error=0.032004\n",
      "epoch 34/35   error=0.030651\n",
      "epoch 35/35   error=0.026606\n",
      "\n",
      "\n",
      "predicted values : \n",
      "[array([[4.66896584e-04, 4.33199805e-06, 1.39545636e-03, 7.24076807e-03,\n",
      "        1.25855722e-04, 3.64985141e-04, 2.26086294e-07, 4.27478927e-01,\n",
      "        1.81274620e-05, 1.43674179e-02]]), array([[1.06176637e-03, 1.36015041e-05, 3.12668704e-03, 5.18160748e-05,\n",
      "        3.93208405e-06, 9.40473081e-01, 2.17117128e-02, 6.98504280e-04,\n",
      "        3.28599992e-05, 1.22415102e-02]]), array([[5.46673315e-05, 9.94146559e-01, 9.59531004e-03, 5.38065084e-03,\n",
      "        7.44338199e-08, 6.74384454e-05, 6.40501976e-04, 3.25442425e-01,\n",
      "        3.12841482e-04, 1.17783046e-03]]), array([[9.52370817e-01, 9.38678289e-08, 1.58089018e-05, 5.00835219e-07,\n",
      "        3.24057727e-08, 9.55969354e-05, 1.52690311e-04, 7.73804612e-03,\n",
      "        1.92294067e-07, 1.26258490e-06]]), array([[2.38608391e-06, 3.66560073e-02, 2.43374933e-05, 3.14614871e-05,\n",
      "        9.93210408e-01, 2.60439042e-06, 9.03847564e-03, 3.06513794e-02,\n",
      "        6.88619193e-04, 4.73527247e-04]]), array([[6.82456874e-06, 9.96105075e-01, 3.17297605e-03, 1.59964986e-03,\n",
      "        3.57181967e-08, 5.55627359e-05, 1.05563320e-04, 4.20627218e-01,\n",
      "        9.97136277e-06, 5.87542357e-03]]), array([[8.85276953e-07, 9.53853543e-05, 8.29862800e-08, 9.73237384e-03,\n",
      "        4.06092897e-01, 2.23255533e-01, 2.42283922e-07, 7.96493605e-02,\n",
      "        8.33753626e-06, 3.48288222e-02]]), array([[1.09161966e-06, 2.49801931e-03, 2.70228212e-03, 1.85964635e-02,\n",
      "        3.30947142e-02, 1.34140776e-03, 9.35777791e-03, 1.50733647e-02,\n",
      "        6.89691679e-05, 1.67947813e-02]]), array([[8.00384987e-03, 7.48472032e-04, 7.24255582e-03, 7.24902187e-08,\n",
      "        2.23029532e-03, 8.32222767e-03, 3.20916927e-02, 1.59473036e-02,\n",
      "        5.32731238e-05, 1.74179576e-07]]), array([[9.23358786e-04, 1.03952421e-05, 1.13187230e-04, 1.86677776e-05,\n",
      "        3.25299625e-05, 1.33850857e-04, 1.44617562e-05, 7.49169743e-01,\n",
      "        9.29893852e-05, 1.69945064e-02]])]\n",
      "true values : \n",
      "[[0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "#from network import Network\n",
    "#from fc_layer import FCLayer\n",
    "#from activation_layer import ActivationLayer\n",
    "#from activations import tanh, tanh_prime\n",
    "#from losses import mse, mse_prime\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from keras.utils import np_utils\n",
    "\n",
    "# load MNIST from server\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "print(x_train.shape)\n",
    "# training data : 60000 samples\n",
    "# reshape and normalize input data\n",
    "x_train = x_train.reshape(x_train.shape[0], 1, 28*28)\n",
    "x_train = x_train.astype('float32')\n",
    "x_train /= 255\n",
    "# encode output which is a number in range [0,9] into a vector of size 10\n",
    "# e.g. number 3 will become [0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
    "y_train = np_utils.to_categorical(y_train)\n",
    "\n",
    "# same for test data : 10000 samples\n",
    "x_test = x_test.reshape(x_test.shape[0], 1, 28*28)\n",
    "x_test = x_test.astype('float32')\n",
    "x_test /= 255\n",
    "y_test = np_utils.to_categorical(y_test)\n",
    "\n",
    "# Network\n",
    "net = Network()\n",
    "net.add(FCLayer(28*28, 100))                # input_shape=(1, 28*28)    ;   output_shape=(1, 100)\n",
    "net.add(ActivationLayer(tanh, tanh_prime))\n",
    "net.add(FCLayer(100, 50))                   # input_shape=(1, 100)      ;   output_shape=(1, 50)\n",
    "net.add(ActivationLayer(relu, relu_prime))\n",
    "net.add(FCLayer(50, 10))                    # input_shape=(1, 50)       ;   output_shape=(1, 10)\n",
    "net.add(ActivationLayer(sigmoid, sigmoid_prime))\n",
    "\n",
    "# train on 1000 samples\n",
    "# as we didn't implemented mini-batch GD, training will be pretty slow if we update at each iteration on 60000 samples...\n",
    "net.use(mse, mse_prime)\n",
    "net.fit(x_train[0:1000], y_train[0:1000], epochs=35, learning_rate=0.1)\n",
    "\n",
    "# test on 3 samples\n",
    "out = net.predict(x_test[0:10])\n",
    "test = net.predict(x_train)\n",
    "print(\"\\n\")\n",
    "print(\"predicted values : \")\n",
    "print(out, end=\"\\n\")\n",
    "print(\"true values : \")\n",
    "print(y_test[0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "ytemp = y_train\n",
    "outtemp = np.array(test)\n",
    "outtemp = outtemp.reshape(ytemp.shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(y, y_hat):\n",
    "    y = np.reshape(y, y_hat.shape)\n",
    "    confusion_matrix = np.zeros((y.shape[-1], y.shape[-1]))\n",
    "    for i in range(y.shape[0]):\n",
    "        prediction = np.argmax(y[i])\n",
    "        true = np.argmax(y_hat[i])\n",
    "        confusion_matrix[true][prediction] += 1\n",
    "    diognal = []\n",
    "    for i in range(confusion_matrix.shape[0]):\n",
    "        diognal.append(confusion_matrix[i, i])\n",
    "    precision_sum = np.sum(confusion_matrix, axis=0)\n",
    "    recall_sum = np.sum(confusion_matrix, axis=1)\n",
    "    all_samples = np.sum(recall_sum)\n",
    "    accuracy = sum(diognal) / all_samples\n",
    "    precision = np.average(diognal / precision_sum)\n",
    "    recall = np.average(diognal / recall_sum)\n",
    "    f1_score = 2 * precision * recall /  (precision + recall)\n",
    "    print(accuracy, precision, recall, f1_score)\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6871666666666667 0.6307356832997879 0.6830839333725578 0.6558669181056928\n"
     ]
    }
   ],
   "source": [
    "evaluate(outtemp, ytemp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "51e6a1bf7724299722fc238a8631cf638aeedbe92c72eb6089b4e713e2df7f82"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
